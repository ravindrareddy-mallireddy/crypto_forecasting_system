{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VB2jmoDO7pxJ"
      },
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from datetime import datetime, date\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/COM724_1/datasets\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "name_to_ticker = {\n",
        "    \"Bitcoin\": \"BTC-USD\", \"Ethereum\": \"ETH-USD\", \"Binance Coin\": \"BNB-USD\", \"Ripple\": \"XRP-USD\", \"Cardano\": \"ADA-USD\", \"Solana\": \"SOL-USD\", \"Polkadot\": \"DOT-USD\",\n",
        "    \"Dogecoin\": \"DOGE-USD\", \"Shiba Inu\": \"SHIB-USD\", \"Litecoin\": \"LTC-USD\", \"Chainlink\": \"LINK-USD\", \"Stellar\": \"XLM-USD\", \"Avalanche\": \"AVAX-USD\", \"Ethereum Classic\": \"ETC-USD\",\n",
        "    \"Monero\": \"XMR-USD\", \"Algorand\": \"ALGO-USD\", \"Cosmos\": \"ATOM-USD\", \"Filecoin\": \"FIL-USD\", \"Internet Computer\": \"ICP-USD\", \"VeChain\": \"VET-USD\", \"Curve DAO Token\": \"CRV-USD\",\n",
        "    \"Aave\": \"AAVE-USD\", \"EOS\": \"EOS-USD\", \"Tezos\": \"XTZ-USD\", \"Maker\": \"MKR-USD\", \"Theta Network\": \"THETA-USD\", \"Axie Infinity\": \"AXS-USD\", \"Decentraland\": \"MANA-USD\",\n",
        "    \"Bitcoin Cash\": \"BCH-USD\", \"Dash\": \"DASH-USD\",\n",
        "}\n",
        "\n",
        "tickers = list(name_to_ticker.values())\n",
        "\n",
        "end_date = datetime.today().date()\n",
        "start_date = (end_date - relativedelta(years=3)).isoformat()\n",
        "end_date = end_date.isoformat()\n",
        "\n",
        "print(f\"Downloading data from {start_date} to {end_date}\")\n",
        "print(f\"Total tickers to process: {len(tickers)}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "successful_downloads = 0\n",
        "\n",
        "for i, ticker in enumerate(tickers, 1):\n",
        "    out_path = OUT_DIR / f\"{ticker}.csv\"\n",
        "    try:\n",
        "        df = yf.download(ticker, start=start_date, end=end_date, interval=\"1d\", progress=False)\n",
        "\n",
        "        if df is None or df.empty:\n",
        "            print(f\"[{i}/{len(tickers)}] ❌ WARNING: No data available for {ticker}\")\n",
        "            continue\n",
        "\n",
        "        if isinstance(df.columns, pd.MultiIndex):\n",
        "            df.columns = [' '.join([str(i) for i in col if str(i) != '']).strip() for col in df.columns.values]\n",
        "\n",
        "        df = df.reset_index()\n",
        "\n",
        "        rename_map = {}\n",
        "        for c in df.columns:\n",
        "            lc = c.lower()\n",
        "            if 'date' in lc:\n",
        "                rename_map[c] = 'Date'\n",
        "            elif lc.startswith('open'):\n",
        "                rename_map[c] = 'Open'\n",
        "            elif lc.startswith('high'):\n",
        "                rename_map[c] = 'High'\n",
        "            elif lc.startswith('low'):\n",
        "                rename_map[c] = 'Low'\n",
        "            elif 'adj' in lc and 'close' in lc:\n",
        "                rename_map[c] = 'Adj Close'\n",
        "            elif lc.startswith('close'):\n",
        "                rename_map[c] = 'Close'\n",
        "            elif 'volume' in lc:\n",
        "                rename_map[c] = 'Volume'\n",
        "\n",
        "        df = df.rename(columns=rename_map)\n",
        "\n",
        "        required = [\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
        "        missing_required = [c for c in required if c not in df.columns]\n",
        "        if missing_required:\n",
        "            print(f\"[{i}/{len(tickers)}] ⚠️  WARNING: {ticker} missing columns {missing_required}\")\n",
        "            continue\n",
        "\n",
        "        df[\"Symbol\"] = ticker\n",
        "        ordered = [\"Date\", \"Symbol\", \"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"]\n",
        "        ordered_present = [c for c in ordered if c in df.columns]\n",
        "        df = df[ordered_present]\n",
        "\n",
        "        df.to_csv(out_path, index=False)\n",
        "        successful_downloads += 1\n",
        "\n",
        "        actual_start = df['Date'].min().strftime('%Y-%m-%d') if len(df) > 0 else 'N/A'\n",
        "        actual_end = df['Date'].max().strftime('%Y-%m-%d') if len(df) > 0 else 'N/A'\n",
        "\n",
        "        print(f\"[{i}/{len(tickers)}] ✅ Saved {ticker} | Rows: {len(df):>4} | Dates: {actual_start} to {actual_end}\")\n",
        "\n",
        "        time.sleep(0.5)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n⚠️  Download interrupted by user.\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"[{i}/{len(tickers)}] ❌ ERROR {ticker}: {e}\")\n",
        "        time.sleep(1)\n",
        "\n",
        "\n",
        "print(f\"Failed downloads: {len(tickers) - successful_downloads}\")\n",
        "\n",
        "saved_files = sorted([p.name for p in OUT_DIR.glob('*.csv')])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "jQiql-DW7wXi",
        "outputId": "748d3819-9687-43fd-a71b-e597a5425762"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1992277219.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0myfinance\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0myf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/COM724_1/datasets\"\n",
        "\n",
        "save_path = \"/content/drive/MyDrive/COM724_1/final_dataset/final_dataset.csv\"\n",
        "\n",
        "all_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
        "\n",
        "merged_list = []\n",
        "\n",
        "for file in all_files:\n",
        "    df = pd.read_csv(file)\n",
        "    merged_list.append(df)\n",
        "\n",
        "final_df = pd.concat(merged_list, ignore_index=True)\n",
        "\n",
        "final_df = final_df.sort_values([\"Symbol\", \"Date\"]).reset_index(drop=True)\n",
        "\n",
        "final_df.to_csv(save_path, index=False)\n",
        "\n",
        "print(f\"✅ Merged {len(all_files)} files.\")\n",
        "print(f\"✅ Final shape: {final_df.shape}\")\n",
        "print(f\"✅ Saved merged dataset to:\\n{save_path}\")"
      ],
      "metadata": {
        "id": "tDhhxegzAnx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "csv_files = [f for f in os.listdir(OUT_DIR) if f.endswith('.csv')]\n",
        "file_row_counts = []\n",
        "\n",
        "for filename in csv_files:\n",
        "    file_path = OUT_DIR / filename\n",
        "    df = pd.read_csv(file_path)\n",
        "    row_count = df.shape[0]\n",
        "    file_row_counts.append({\"Filename\": filename, \"Row Count\": row_count})\n",
        "row_counts_df = pd.DataFrame(file_row_counts)\n",
        "display(row_counts_df)"
      ],
      "metadata": {
        "id": "m3nvAHqo-Jbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data preprocessing\n"
      ],
      "metadata": {
        "id": "XcG9gikKBn7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"length of dataset {len(final_df)}\")\n",
        "missing_values = final_df.isnull().sum()\n",
        "print(\"Missing values:\")\n",
        "print(missing_values)\n",
        "\n",
        "duplicate_rows = final_df.duplicated().sum()\n",
        "print(f\"\\nNumber of duplicate rows: {duplicate_rows}\")"
      ],
      "metadata": {
        "id": "ThSBS9sGBnjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.info()"
      ],
      "metadata": {
        "id": "E2RPFpudMpLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "270e32cc"
      },
      "source": [
        "final_df['Date'] = pd.to_datetime(final_df['Date'])\n",
        "\n",
        "final_df['Volume'] = final_df['Volume'].astype(float)\n",
        "\n",
        "name_to_ticker_inverse = {v: k for k, v in name_to_ticker.items()}\n",
        "final_df['Name'] = final_df['Symbol'].map(name_to_ticker_inverse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "output_dir = Path(\"/content/drive/MyDrive/COM724_1/final_dataset\")\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "output_file_path = output_dir / \"final_df.csv\"\n",
        "\n",
        "final_df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(f\"✅ Saved preprocessed final_df to: {output_file_path}\")"
      ],
      "metadata": {
        "id": "G-33WdNlSSnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EDA"
      ],
      "metadata": {
        "id": "svhMXkXISAMT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cy3qKqzcQGqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da2bb1c2"
      },
      "source": [
        "print(f\"Total rows: {final_df.shape[0]}\")\n",
        "print(f\"Total columns: {final_df.shape[1]}\")\n",
        "\n",
        "min_date = final_df['Date'].min().strftime('%Y-%m-%d')\n",
        "max_date = final_df['Date'].max().strftime('%Y-%m-%d')\n",
        "print(f\"\\nData period: from {min_date} to {max_date}\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "337ecc2f"
      },
      "source": [
        "coin_stats = final_df.groupby('Symbol')[['Open', 'High', 'Low', 'Close', 'Volume']].describe()\n",
        "\n",
        "print(\"Descriptive statistics for each coin:\")\n",
        "display(coin_stats)\n",
        "\n",
        "most_volatile_coin = coin_stats['Close']['std'].idxmax()\n",
        "highest_volatility_std = coin_stats['Close']['std'].max()\n",
        "\n",
        "print(f\"\\nMost volatile coin (based on standard deviation of Close price): {most_volatile_coin} (Std Dev: {highest_volatility_std:.2f})\")\n",
        "\n",
        "most_liquid_coin = coin_stats['Volume']['mean'].idxmax()\n",
        "highest_average_volume = coin_stats['Volume']['mean'].max()\n",
        "\n",
        "print(f\"Coin with highest average volume (liquidity): {most_liquid_coin} (Average Volume: {highest_average_volume:.2f})\")\n",
        "\n",
        "highest_average_price_coin = coin_stats['Close']['mean'].idxmax()\n",
        "highest_average_price = coin_stats['Close']['mean'].max()\n",
        "\n",
        "print(f\"Coin with highest average close price: {highest_average_price_coin} (Average Close Price: {highest_average_price:.2f})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "407625ec"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "selected_coins = ['BTC-USD', 'ETH-USD', 'DOGE-USD']\n",
        "filtered_df = final_df[final_df['Symbol'].isin(selected_coins)].copy()\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "sns.lineplot(data=filtered_df, x='Date', y='Close', hue='Name')\n",
        "plt.title('Close Price Trend for BTC, ETH, and DOGE (Past 3 Years)')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close Price (USD)')\n",
        "plt.grid(True)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correlation_matrix = final_df.select_dtypes(include=['float64', 'int64']).corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Heatmap of Numerical Features')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WGQ2F3-qu_Gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b44dd30"
      },
      "source": [
        "btc_df = final_df[final_df['Symbol'] == 'BTC-USD'].copy()\n",
        "print(f\"Shape of BTC-USD DataFrame: {btc_df.shape}\")\n",
        "btc_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e0f1be4"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "numerical_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
        "btc_numerical_df = btc_df[numerical_cols]\n",
        "\n",
        "btc_correlation_matrix = btc_numerical_df.corr()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(btc_correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Matrix for BTC-USD Numerical Features')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "OUT_DIR = \"data\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "if 'final_df' not in globals():\n",
        "    raise RuntimeError(\"final_df not found. Run feature engineering / load your master dataframe first.\")\n",
        "\n",
        "if 'pct_change' in final_df.columns:\n",
        "    ret_col = 'pct_change'\n",
        "elif 'log_return' in final_df.columns:\n",
        "    ret_col = 'log_return'\n",
        "else:\n",
        "    final_df = final_df.sort_values(['Symbol','Date']).copy()\n",
        "    final_df['pct_change'] = final_df.groupby('Symbol')['Close'].pct_change()\n",
        "    ret_col = 'pct_change'\n",
        "\n",
        "pivot = final_df.pivot_table(index='Date', columns='Symbol', values=ret_col, aggfunc='first')\n",
        "\n",
        "thresh = int(0.70 * pivot.shape[0])\n",
        "pivot_clean = pivot.dropna(axis=1, thresh=thresh).copy()\n",
        "\n",
        "pivot_clean = pivot_clean.fillna(0)\n",
        "\n",
        "corr = pivot_clean.corr(method='pearson')\n",
        "\n",
        "corr.to_csv(os.path.join(OUT_DIR, \"correlation_matrix.csv\"))\n",
        "\n",
        "summary_rows = []\n",
        "symbols = corr.index.tolist()\n",
        "\n",
        "for s in symbols:\n",
        "    row = corr[s].drop(labels=[s])\n",
        "    sorted_pos = row.sort_values(ascending=False)\n",
        "    top4_pos = sorted_pos.head(4).index.tolist()\n",
        "    top4_pos_vals = sorted_pos.head(4).values.tolist()\n",
        "    top4_neg = sorted_pos.tail(4).index.tolist()[::-1]\n",
        "    top4_neg_vals = sorted_pos.tail(4).values.tolist()[::-1]\n",
        "    summary_rows.append({\n",
        "        'Symbol': s,\n",
        "        'top1_pos': top4_pos[0] if len(top4_pos)>0 else None,\n",
        "        'top1_pos_val': top4_pos_vals[0] if len(top4_pos_vals)>0 else None,\n",
        "        'top2_pos': top4_pos[1] if len(top4_pos)>1 else None,\n",
        "        'top2_pos_val': top4_pos_vals[1] if len(top4_pos_vals)>1 else None,\n",
        "        'top3_pos': top4_pos[2] if len(top4_pos)>2 else None,\n",
        "        'top3_pos_val': top4_pos_vals[2] if len(top4_pos_vals)>2 else None,\n",
        "        'top4_pos': top4_pos[3] if len(top4_pos)>3 else None,\n",
        "        'top4_pos_val': top4_pos_vals[3] if len(top4_pos_vals)>3 else None,\n",
        "        'top1_neg': top4_neg[0] if len(top4_neg)>0 else None,\n",
        "        'top1_neg_val': top4_neg_vals[0] if len(top4_neg_vals)>0 else None,\n",
        "        'top2_neg': top4_neg[1] if len(top4_neg)>1 else None,\n",
        "        'top2_neg_val': top4_neg_vals[1] if len(top4_neg_vals)>1 else None,\n",
        "        'top3_neg': top4_neg[2] if len(top4_neg)>2 else None,\n",
        "        'top3_neg_val': top4_neg_vals[2] if len(top4_neg_vals)>2 else None,\n",
        "        'top4_neg': top4_neg[3] if len(top4_neg)>3 else None,\n",
        "        'top4_neg_val': top4_neg_vals[3] if len(top4_neg_vals)>3 else None,\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_rows)\n",
        "summary_df.to_csv(os.path.join(OUT_DIR, \"correlation_summary.csv\"), index=False)\n",
        "\n",
        "print(\"Saved correlation_matrix.csv and correlation_summary.csv to\", OUT_DIR)\n",
        "print(\"Sample (first 5 rows):\")\n",
        "display(summary_df.head())\n",
        "\n",
        "try:\n",
        "    import scipy.cluster.hierarchy as sch\n",
        "\n",
        "    linkage = sch.linkage(1 - corr.fillna(0).values, method='average')\n",
        "    dendro = sch.dendrogram(linkage, no_plot=True)\n",
        "    order = dendro['leaves']\n",
        "    ordered_corr = corr.iloc[order, order]\n",
        "\n",
        "    plt.figure(figsize=(10,8))\n",
        "    sns.heatmap(ordered_corr, cmap='vlag', center=0, xticklabels=True, yticklabels=True, square=False)\n",
        "    plt.title(\"Clustered correlation heatmap (returns)\")\n",
        "    plt.tight_layout()\n",
        "    heatmap_path = os.path.join(OUT_DIR, \"correlation_heatmap.png\")\n",
        "    plt.savefig(heatmap_path, dpi=150)\n",
        "    plt.show()\n",
        "    print(\"Saved heatmap to\", heatmap_path)\n",
        "except Exception as e:\n",
        "    print(\"Heatmap skipped (scipy may be missing or clustering failed):\", e)\n",
        "\n",
        "def plot_coin_correlation(symbol, top_n=10):\n",
        "    if symbol not in corr.columns:\n",
        "        raise KeyError(f\"{symbol} not found in correlation matrix.\")\n",
        "    series = corr[symbol].drop(labels=[symbol]).sort_values(ascending=False)\n",
        "    top = pd.concat([series.head(top_n), series.tail(top_n)])\n",
        "    plt.figure(figsize=(8,4))\n",
        "    top.plot(kind='bar', color=['tab:blue' if v>=0 else 'tab:red' for v in top.values])\n",
        "    plt.title(f\"Top correlations for {symbol} (positive -> negative)\")\n",
        "    plt.ylabel(\"Pearson correlation\")\n",
        "    plt.axhline(0, color='k', linewidth=0.8)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if 'selected_coins' in globals() and len(selected_coins)>0:\n",
        "    plot_coin_correlation(selected_coins[0], top_n=8)"
      ],
      "metadata": {
        "id": "HEhZK4as-3b1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.info()"
      ],
      "metadata": {
        "id": "WmlxCWRHVB8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#feature engineering"
      ],
      "metadata": {
        "id": "-5hYPI9JVaxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "def _rsi(series, window=14):\n",
        "    delta = series.diff()\n",
        "    up = delta.clip(lower=0)\n",
        "    down = (-delta).clip(lower=0)\n",
        "    ma_up = up.rolling(window=window, min_periods=1).mean()\n",
        "    ma_down = down.rolling(window=window, min_periods=1).mean()\n",
        "    rs = ma_up / (ma_down + 1e-12)\n",
        "    return 100 - (100 / (1 + rs))\n",
        "\n",
        "\n",
        "def _macd(close, n_fast=12, n_slow=26, n_signal=9):\n",
        "    ema_fast = close.ewm(span=n_fast, adjust=False).mean()\n",
        "    ema_slow = close.ewm(span=n_slow, adjust=False).mean()\n",
        "    macd = ema_fast - ema_slow\n",
        "    signal = macd.ewm(span=n_signal, adjust=False).mean()\n",
        "    hist = macd - signal\n",
        "    return macd, signal, hist\n",
        "\n",
        "\n",
        "def _atr(high, low, close, window=14):\n",
        "    prev = close.shift(1)\n",
        "    tr1 = high - low\n",
        "    tr2 = (high - prev).abs()\n",
        "    tr3 = (low - prev).abs()\n",
        "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
        "    return tr.rolling(window=window, min_periods=1).mean()\n",
        "\n",
        "\n",
        "def _rolling_max_drawdown(series, window=30):\n",
        "    roll_max = series.rolling(window=window, min_periods=1).max()\n",
        "    dd = series / roll_max - 1\n",
        "    return dd.rolling(window=window, min_periods=1).min()\n",
        "\n",
        "\n",
        "\n",
        "final_df['Date'] = pd.to_datetime(final_df['Date'], errors='coerce')\n",
        "for col in ['Open','High','Low','Close','Volume']:\n",
        "    final_df[col] = pd.to_numeric(final_df[col], errors='coerce')\n",
        "\n",
        "final_df = final_df.sort_values(['Symbol','Date']).reset_index(drop=True)\n",
        "\n",
        "grp = final_df.groupby(\"Symbol\", group_keys=False)\n",
        "\n",
        "final_df['pct_change']  = grp['Close'].pct_change()\n",
        "final_df['log_return']  = np.log(final_df['Close']).diff()\n",
        "\n",
        "final_df['lag_1'] = grp['Close'].shift(1)\n",
        "final_df['lag_7'] = grp['Close'].shift(7)\n",
        "\n",
        "windows = [7,14,21]\n",
        "for w in windows:\n",
        "    final_df[f'roll_mean_{w}'] = grp['Close'].transform(lambda s: s.rolling(w, min_periods=1).mean())\n",
        "    final_df[f'roll_std_{w}']  = grp['Close'].transform(lambda s: s.rolling(w, min_periods=1).std(ddof=0)).fillna(0)\n",
        "    final_df[f'roll_max_{w}']  = grp['Close'].transform(lambda s: s.rolling(w, min_periods=1).max())\n",
        "    final_df[f'roll_min_{w}']  = grp['Close'].transform(lambda s: s.rolling(w, min_periods=1).min())\n",
        "    final_df[f'volatility_{w}'] = grp['pct_change'].transform(\n",
        "        lambda s: s.rolling(w, min_periods=1).std().fillna(0) * np.sqrt(252)\n",
        "    )\n",
        "\n",
        "final_df['rsi_14'] = grp['Close'].transform(lambda s: _rsi(s, 14))\n",
        "final_df['atr_14'] = grp.apply(lambda g: _atr(g['High'], g['Low'], g['Close'], 14)).reset_index(level=0, drop=True)\n",
        "\n",
        "final_df['macd'] = np.nan\n",
        "final_df['macd_signal'] = np.nan\n",
        "final_df['macd_hist'] = np.nan\n",
        "for sym, g in final_df.groupby('Symbol'):\n",
        "    m, sig, h = _macd(g['Close'])\n",
        "    final_df.loc[g.index, 'macd'] = m.values\n",
        "    final_df.loc[g.index, 'macd_signal'] = sig.values\n",
        "    final_df.loc[g.index, 'macd_hist'] = h.values\n",
        "\n",
        "final_df['drawdown_30'] = grp['Close'].transform(lambda s: _rolling_max_drawdown(s, 30))\n",
        "\n",
        "final_df['vol_mean_7'] = grp['Volume'].transform(lambda s: s.rolling(7, min_periods=1).mean())\n",
        "final_df['vol_mean_21'] = grp['Volume'].transform(lambda s: s.rolling(21, min_periods=1).mean())\n",
        "\n",
        "final_df['target_next_close'] = grp['Close'].shift(-1)\n",
        "final_df['target_next_pct'] = final_df['target_next_close'] / final_df['Close'] - 1\n",
        "\n",
        "final_df = final_df.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "print(\"Feature engineering completed.\")\n",
        "print(\"Rows:\", final_df.shape)\n",
        "print(\"Missing values:\", final_df.isna().sum().sort_values(ascending=False).head(10))\n"
      ],
      "metadata": {
        "id": "KY0yBEVIVdIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.info()"
      ],
      "metadata": {
        "id": "qFLAgT6bVikl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = final_df.sort_values([\"Symbol\", \"Date\"]).reset_index(drop=True)\n",
        "\n",
        "final_df[\"row_num\"] = final_df.groupby(\"Symbol\").cumcount()\n",
        "final_df = final_df[final_df[\"row_num\"] >= 30]\n",
        "\n",
        "final_df[\"row_num\"] = final_df.groupby(\"Symbol\").cumcount()\n",
        "final_df[\"max_row\"] = final_df.groupby(\"Symbol\")[\"row_num\"].transform(\"max\")\n",
        "final_df = final_df[final_df[\"row_num\"] != final_df[\"max_row\"]]\n",
        "\n",
        "final_df[\"volatility_21\"] = final_df.groupby(\"Symbol\")[\"volatility_21\"].ffill()\n",
        "\n",
        "final_df = final_df.drop(columns=[\"row_num\", \"max_row\"])\n",
        "\n",
        "final_df = final_df.dropna()\n",
        "\n",
        "print(\"After cleaning, rows:\", final_df.shape)\n",
        "print(\"Number of coins:\", final_df[\"Symbol\"].nunique())\n",
        "print(final_df[\"Symbol\"].value_counts().head())\n",
        "print(\"Remaining NaNs:\", final_df.isna().sum().sum())\n"
      ],
      "metadata": {
        "id": "hMzjJYn5WJLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.info()"
      ],
      "metadata": {
        "id": "_SIFLtvmWMSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#clustering"
      ],
      "metadata": {
        "id": "JLZDSFNzWjVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "numeric_cols = final_df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "\n",
        "remove_cols = ['target_next_close', 'target_next_pct']\n",
        "numeric_cols = [c for c in numeric_cols if c not in remove_cols]\n",
        "\n",
        "final_df_agg = final_df.groupby('Symbol')[numeric_cols].agg(['mean','std'])\n",
        "\n",
        "final_df_agg.columns = ['_'.join(col).strip() for col in final_df_agg.columns]\n",
        "final_df_agg = final_df_agg.reset_index()\n",
        "\n",
        "print(\"Aggregated shape:\", final_df_agg.shape)\n",
        "print(final_df_agg.head(3))\n"
      ],
      "metadata": {
        "id": "2MKPKA4qWjE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sharpe_auto = final_df.groupby('Symbol').apply(\n",
        "    lambda g: pd.Series({\n",
        "        'sharpe_ratio': g['pct_change'].mean() / (g['pct_change'].std() + 1e-9),\n",
        "        'autocorr1': g['pct_change'].autocorr(1)\n",
        "    })\n",
        ").reset_index()\n",
        "\n",
        "final_df_agg = final_df_agg.merge(sharpe_auto, on='Symbol', how='left')\n",
        "\n",
        "print(\"Shape after Sharpe/Autocorr:\", final_df_agg.shape)\n"
      ],
      "metadata": {
        "id": "paWO9MF0WoRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X = final_df_agg.drop(columns=['Symbol'])\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n"
      ],
      "metadata": {
        "id": "pjTCCoveWq9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "n_components = min(5, X_scaled.shape[1])\n",
        "pca = PCA(n_components=n_components, random_state=42)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "print(\"PCA components used:\", n_components)\n",
        "print(\"Explained variance:\", pca.explained_variance_ratio_)\n"
      ],
      "metadata": {
        "id": "J3tFZjkoWtPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans = KMeans(n_clusters=4, random_state=42, n_init=20)\n",
        "clusters = kmeans.fit_predict(X_pca)\n",
        "\n",
        "final_df_agg['cluster'] = clusters\n",
        "\n",
        "print(final_df_agg[['Symbol','cluster']])\n"
      ],
      "metadata": {
        "id": "ByaP-KJNWwWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "try:\n",
        "    final_df_agg\n",
        "except NameError:\n",
        "    numeric_cols = final_df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "    remove_cols = [c for c in ['target_next_close','target_next_pct'] if c in numeric_cols]\n",
        "    numeric_cols = [c for c in numeric_cols if c not in remove_cols]\n",
        "    final_df_agg = final_df.groupby('Symbol')[numeric_cols].agg(['mean','std'])\n",
        "    final_df_agg.columns = ['_'.join(col).strip() for col in final_df_agg.columns]\n",
        "    final_df_agg = final_df_agg.reset_index()\n",
        "    extra = final_df.groupby('Symbol').apply(lambda g: pd.Series({\n",
        "        'sharpe_ratio': g['pct_change'].mean() / (g['pct_change'].std() + 1e-9),\n",
        "        'autocorr1': g['pct_change'].autocorr(1)\n",
        "    })).reset_index()\n",
        "    final_df_agg = final_df_agg.merge(extra, on='Symbol', how='left')\n",
        "\n",
        "X_all = final_df_agg.drop(columns=['Symbol']).select_dtypes(include=[np.number]).fillna(0)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_all)\n",
        "\n",
        "n_pca = min(10, X_scaled.shape[1])\n",
        "pca = PCA(n_components=n_pca, random_state=42)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "kmeans = KMeans(n_clusters=4, random_state=42, n_init=20)\n",
        "clusters = kmeans.fit_predict(X_pca[:, :min(5, X_pca.shape[1])])\n",
        "final_df_agg['cluster'] = clusters\n",
        "\n",
        "sil = silhouette_score(X_pca[:, :min(5, X_pca.shape[1])], clusters) if X_pca.shape[0] > len(np.unique(clusters)) else np.nan\n",
        "print(f\"Silhouette score (used space): {sil:.4f}\")\n",
        "\n",
        "pc1 = X_pca[:,0]\n",
        "z = (pc1 - pc1.mean()) / (pc1.std() + 1e-12)\n",
        "\n",
        "outlier_mask = np.abs(z) > 3\n",
        "outliers = final_df_agg['Symbol'].values[outlier_mask]\n",
        "print(\"Outliers detected (excluded from certain plots):\", outliers.tolist())\n",
        "\n",
        "pc_plot = X_pca[:, :3] if X_pca.shape[1] >= 3 else np.hstack([X_pca, np.zeros((X_pca.shape[0], 3 - X_pca.shape[1]))])\n",
        "centers = []\n",
        "for c in sorted(np.unique(clusters)):\n",
        "    idx = np.where(clusters == c)[0]\n",
        "    centers.append(pc_plot[idx, :2].mean(axis=0))\n",
        "centers = np.array(centers)\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18,5))\n",
        "cmap = plt.get_cmap('tab10')\n",
        "\n",
        "mask = ~outlier_mask\n",
        "ax = axes[0]\n",
        "scatter = ax.scatter(pc_plot[mask,0], pc_plot[mask,1], c=clusters[mask], cmap=cmap, s=80, alpha=0.9, edgecolors='k', linewidths=0.4)\n",
        "ax.scatter(centers[:,0], centers[:,1], marker='X', s=200, c='black', label='centroids')\n",
        "ax.set_title('PCA 1 vs PCA 2 (outliers removed)')\n",
        "ax.set_xlabel('PCA1'); ax.set_ylabel('PCA2')\n",
        "\n",
        "for c in sorted(np.unique(clusters)):\n",
        "    subset = final_df_agg[final_df_agg['cluster']==c].copy()\n",
        "    vol_cols = [col for col in final_df_agg.columns if 'Volume_mean' in col]\n",
        "    if len(vol_cols)>0:\n",
        "        rep = subset.sort_values(vol_cols[0], ascending=False).iloc[0]['Symbol']\n",
        "    else:\n",
        "        rep = subset.iloc[0]['Symbol']\n",
        "    idx = final_df_agg.index[final_df_agg['Symbol']==rep][0]\n",
        "    ax.annotate(rep, (pc_plot[idx,0], pc_plot[idx,1]), fontsize=9, fontweight='bold', xytext=(6,3), textcoords='offset points')\n",
        "\n",
        "ax.legend(*scatter.legend_elements(), title=\"cluster\", loc='upper right', bbox_to_anchor=(1.4,1))\n",
        "ax.grid(alpha=0.25)\n",
        "\n",
        "ax = axes[1]\n",
        "if pc_plot.shape[1] >= 3:\n",
        "    ax.scatter(pc_plot[mask,1], pc_plot[mask,2], c=clusters[mask], cmap=cmap, s=80, alpha=0.9, edgecolors='k', linewidths=0.4)\n",
        "    ax.set_title('PCA 2 vs PCA 3 (outliers removed)')\n",
        "    ax.set_xlabel('PCA2'); ax.set_ylabel('PCA3')\n",
        "    for c in sorted(np.unique(clusters)):\n",
        "        rep = final_df_agg[final_df_agg['cluster']==c].iloc[0]['Symbol']\n",
        "        idx = final_df_agg.index[final_df_agg['Symbol']==rep][0]\n",
        "        ax.annotate(rep, (pc_plot[idx,1], pc_plot[idx,2]), fontsize=8)\n",
        "    ax.grid(alpha=0.25)\n",
        "else:\n",
        "    ax.text(0.5, 0.5, \"Not enough PCA dims for PC3\", ha='center')\n",
        "\n",
        "ax = axes[2]\n",
        "tsne = TSNE(n_components=2, init='pca', random_state=42, perplexity=8)\n",
        "X_tsne = tsne.fit_transform(X_scaled)\n",
        "ax.scatter(X_tsne[mask,0], X_tsne[mask,1], c=clusters[mask], cmap=cmap, s=80, alpha=0.9, edgecolors='k', linewidths=0.4)\n",
        "ax.set_title('t-SNE (outliers removed)')\n",
        "ax.set_xlabel('t-SNE 1'); ax.set_ylabel('t-SNE 2')\n",
        "for c in sorted(np.unique(clusters)):\n",
        "    subset = final_df_agg[final_df_agg['cluster']==c].copy()\n",
        "    rep = subset.iloc[0]['Symbol']\n",
        "    idx = final_df_agg.index[final_df_agg['Symbol']==rep][0]\n",
        "    ax.annotate(rep, (X_tsne[idx,0], X_tsne[idx,1]), fontsize=9)\n",
        "\n",
        "ax.grid(alpha=0.25)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0cjhO33PXGZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta = final_df.groupby(\"Symbol\").agg(\n",
        "    avg_volume = (\"Volume\", \"mean\"),\n",
        "    obs_count = (\"Date\", \"count\"),\n",
        "    avg_drawdown = (\"drawdown_30\", \"mean\")\n",
        ").reset_index()\n",
        "\n",
        "\n",
        "df_clusters = final_df_agg[[\"Symbol\", \"cluster\"]].merge(meta, on=\"Symbol\", how=\"left\")\n",
        "\n",
        "\n",
        "selected_coins = []\n",
        "selection_details = []\n",
        "\n",
        "for cl in sorted(df_clusters[\"cluster\"].unique()):\n",
        "    group = df_clusters[df_clusters[\"cluster\"] == cl].copy()\n",
        "\n",
        "    group = group.sort_values(\n",
        "        [\"avg_volume\", \"obs_count\", \"avg_drawdown\"],\n",
        "        ascending=[False, False, True]\n",
        "    )\n",
        "\n",
        "    best_coin = group.iloc[0][\"Symbol\"]\n",
        "    selected_coins.append(best_coin)\n",
        "\n",
        "    selection_details.append({\n",
        "        \"cluster\": cl,\n",
        "        \"selected_coin\": best_coin,\n",
        "        \"avg_volume\": group.iloc[0][\"avg_volume\"],\n",
        "        \"obs_count\": group.iloc[0][\"obs_count\"],\n",
        "        \"avg_drawdown\": group.iloc[0][\"avg_drawdown\"]\n",
        "    })\n",
        "\n",
        "\n",
        "selected_df = pd.DataFrame(selection_details)\n",
        "\n",
        "print(\"\\n===== SELECTED REPRESENTATIVE COINS =====\")\n",
        "print(selected_df)\n",
        "\n",
        "print(\"\\nFinal list of coins:\")\n",
        "print(selected_coins)\n"
      ],
      "metadata": {
        "id": "Ez-oXD2IZKiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#cluster output files"
      ],
      "metadata": {
        "id": "lMzlQn55m17u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/COM724_1/outputs/clustering\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Saving outputs to:\", OUT_DIR)\n",
        "\n",
        "cluster_labels_df = pd.DataFrame({\n",
        "    \"symbol\": final_df_agg[\"Symbol\"].values,\n",
        "    \"cluster\": final_df_agg[\"cluster\"].values,\n",
        "    \"pca_1\": X_pca[:, 0],\n",
        "    \"pca_2\": X_pca[:, 1],\n",
        "})\n",
        "\n",
        "cluster_labels_df.to_csv(OUT_DIR / \"cluster_labels.csv\", index=False)\n",
        "\n",
        "cluster_groups = (\n",
        "    cluster_labels_df\n",
        "    .groupby(\"cluster\")[\"symbol\"]\n",
        "    .apply(list)\n",
        "    .to_dict()\n",
        ")\n",
        "\n",
        "with open(OUT_DIR / \"cluster_groups.json\", \"w\") as f:\n",
        "    json.dump(cluster_groups, f, indent=2)\n",
        "\n",
        "rep_df = pd.DataFrame({\n",
        "    \"cluster\": list(range(len(selected_coins))),\n",
        "    \"representative_coin\": selected_coins\n",
        "})\n",
        "\n",
        "rep_df.to_csv(OUT_DIR / \"cluster_representatives.csv\", index=False)\n",
        "\n",
        "selected_df.to_csv(\n",
        "    OUT_DIR / \"cluster_representatives_reasoning.csv\",\n",
        "    index=False\n",
        ")\n",
        "\n",
        "if \"corr\" in globals():\n",
        "    corr_matrix = corr.copy()\n",
        "else:\n",
        "    tmp = final_df.copy()\n",
        "    tmp.columns = tmp.columns.str.lower()\n",
        "\n",
        "    tmp[\"returns\"] = (\n",
        "        tmp.sort_values(\"date\")\n",
        "           .groupby(\"symbol\")[\"close\"]\n",
        "           .pct_change()\n",
        "    )\n",
        "\n",
        "    pivot = tmp.pivot(\n",
        "        index=\"date\", columns=\"symbol\", values=\"returns\"\n",
        "    )\n",
        "    corr_matrix = pivot.corr()\n",
        "\n",
        "for coin in rep_df[\"representative_coin\"]:\n",
        "    if coin not in corr_matrix.columns:\n",
        "        continue\n",
        "\n",
        "    corr_series = (\n",
        "        corr_matrix[coin]\n",
        "        .drop(index=coin)\n",
        "        .sort_values(ascending=False)\n",
        "    )\n",
        "\n",
        "    corr_series.to_csv(\n",
        "        OUT_DIR / f\"cluster_correlation_{coin}.csv\",\n",
        "        header=[\"correlation\"]\n",
        "    )\n",
        "\n",
        "print(\"All REQUIRED AE2 clustering outputs generated successfully.\")\n",
        "print(\"Files created:\")\n",
        "for f in sorted(OUT_DIR.iterdir()):\n",
        "    print(\" -\", f.name)\n"
      ],
      "metadata": {
        "id": "MY7e_X0Ym1lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rep_df.head()"
      ],
      "metadata": {
        "id": "Jf1TUaX70qhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_coins = rep_df['representative_coin'].unique()\n",
        "\n",
        "df_4coins = final_df[final_df['Symbol'].isin(selected_coins)].copy()\n",
        "\n",
        "df_4coins = df_4coins.sort_values(['Symbol', 'Date']).reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "aipL2aSa1iFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of rows for AVAX-USD: {len(df_4coins[df_4coins['Symbol'] == 'AVAX-USD'])}\")"
      ],
      "metadata": {
        "id": "KFhJww9U1vdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.columns"
      ],
      "metadata": {
        "id": "kZBj6CYb3Wwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#train/test split"
      ],
      "metadata": {
        "id": "z8iea8f2zlfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_4coins['Date'] = pd.to_datetime(df_4coins['Date'])\n",
        "df_4coins = df_4coins.sort_values(['Symbol', 'Date']).reset_index(drop=True)\n",
        "\n",
        "train_ratio = 0.7\n",
        "val_ratio = 0.15\n",
        "test_ratio = 0.15\n",
        "\n",
        "train_list = []\n",
        "val_list = []\n",
        "test_list = []\n",
        "\n",
        "for coin, g in df_4coins.groupby('Symbol'):\n",
        "    n = len(g)\n",
        "    train_end = int(n * train_ratio)\n",
        "    val_end = int(n * (train_ratio + val_ratio))\n",
        "\n",
        "    train_list.append(g.iloc[:train_end])\n",
        "    val_list.append(g.iloc[train_end:val_end])\n",
        "    test_list.append(g.iloc[val_end:])\n",
        "\n",
        "train_df = pd.concat(train_list).reset_index(drop=True)\n",
        "val_df = pd.concat(val_list).reset_index(drop=True)\n",
        "test_df = pd.concat(test_list).reset_index(drop=True)\n",
        "\n",
        "print(\"Train:\", train_df.shape)\n",
        "print(\"Validation:\", val_df.shape)\n",
        "print(\"Test:\", test_df.shape)\n"
      ],
      "metadata": {
        "id": "OQtnjEWg342S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y2taJk2iz295"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#baseline model"
      ],
      "metadata": {
        "id": "QP_q6k-KbL0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "feature_cols = [\n",
        "    'lag_1',\n",
        "    'lag_7',\n",
        "    'roll_mean_7',\n",
        "    'roll_std_7',\n",
        "    'volatility_7'\n",
        "]\n",
        "\n",
        "target_col = 'target_next_close'\n",
        "\n",
        "results = []\n",
        "\n",
        "for coin in train_df['Symbol'].unique():\n",
        "\n",
        "    train_c = train_df[train_df['Symbol'] == coin]\n",
        "    val_c   = val_df[val_df['Symbol'] == coin]\n",
        "    test_c  = test_df[test_df['Symbol'] == coin]\n",
        "\n",
        "    X_train = train_c[feature_cols]\n",
        "    y_train = train_c[target_col]\n",
        "\n",
        "    X_val = val_c[feature_cols]\n",
        "    y_val = val_c[target_col]\n",
        "\n",
        "    X_test = test_c[feature_cols]\n",
        "    y_test = test_c[target_col]\n",
        "\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_test_pred = model.predict(X_test)\n",
        "\n",
        "    mae = mean_absolute_error(y_test, y_test_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "\n",
        "    results.append({\n",
        "        'Symbol': coin,\n",
        "        'MAE': mae,\n",
        "        'RMSE': rmse\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df\n"
      ],
      "metadata": {
        "id": "aLwhNSPm4ZYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "X_test = test_df[feature_cols]\n",
        "y_test = test_df[target_col]\n",
        "\n",
        "\n",
        "y_test_pred = lr.predict(X_test)\n",
        "\n",
        "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
        "\n",
        "print(\"Baseline Linear Regression - TEST SET\")\n",
        "print(f\"MAE  : {test_mae:.4f}\")\n",
        "print(f\"RMSE : {test_rmse:.4f}\")\n"
      ],
      "metadata": {
        "id": "xP9VhcQA4nUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#random forest"
      ],
      "metadata": {
        "id": "LjZIGOlA7NCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "BASE_PATH = \"/content/drive/MyDrive/COM724_1/models\"\n",
        "os.makedirs(BASE_PATH, exist_ok=True)\n",
        "\n",
        "feature_cols = [\n",
        "    'lag_1',\n",
        "    'lag_7',\n",
        "    'roll_mean_7',\n",
        "    'roll_std_7',\n",
        "    'volatility_7',\n",
        "    'rsi_14',\n",
        "    'macd',\n",
        "    'macd_signal'\n",
        "]\n",
        "\n",
        "target_col = 'target_next_close'\n",
        "MAX_FORECAST_DAYS = 180   # 6 months\n",
        "coins = rep_df['representative_coin'].unique()\n",
        "\n",
        "def forecast_next_days(model, last_row, start_date, n_days):\n",
        "    forecasts = []\n",
        "    current = last_row.copy()\n",
        "    current_date = pd.to_datetime(start_date)\n",
        "\n",
        "    for step in range(1, n_days + 1):\n",
        "        X_step = pd.DataFrame([current[feature_cols]])\n",
        "        pred_close = model.predict(X_step)[0]\n",
        "\n",
        "        current_date += pd.Timedelta(days=1)\n",
        "\n",
        "        forecasts.append({\n",
        "            'Date': current_date,\n",
        "            'Day_Number': step,\n",
        "            'Forecast_Close': pred_close\n",
        "        })\n",
        "\n",
        "        current['lag_7'] = current['lag_1']\n",
        "        current['lag_1'] = pred_close\n",
        "\n",
        "        current['roll_mean_7'] = (current['roll_mean_7'] * 6 + pred_close) / 7\n",
        "\n",
        "    return pd.DataFrame(forecasts)\n",
        "\n",
        "for coin in coins:\n",
        "    coin_df = df_4coins[df_4coins['Symbol'] == coin].copy()\n",
        "    coin_df = coin_df.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "    X = coin_df[feature_cols]\n",
        "    y = coin_df[target_col]\n",
        "\n",
        "    rf = RandomForestRegressor(\n",
        "        n_estimators=300,\n",
        "        max_depth=10,\n",
        "        min_samples_leaf=5,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    rf.fit(X, y)\n",
        "\n",
        "    coin_df['rf_predicted_close'] = rf.predict(X)\n",
        "\n",
        "    past_path = f\"{BASE_PATH}/{coin}_rf_past_predictions.csv\"\n",
        "    coin_df[['Date', 'Symbol', 'Close', 'rf_predicted_close']].to_csv(\n",
        "        past_path, index=False\n",
        "    )\n",
        "\n",
        "    last_row = coin_df.iloc[-1]\n",
        "    last_date = coin_df['Date'].iloc[-1]\n",
        "\n",
        "    forecast_df = forecast_next_days(\n",
        "        model=rf,\n",
        "        last_row=last_row,\n",
        "        start_date=last_date,\n",
        "        n_days=MAX_FORECAST_DAYS\n",
        "    )\n",
        "\n",
        "    forecast_df['Symbol'] = coin\n",
        "\n",
        "    forecast_path = f\"{BASE_PATH}/{coin}_rf_forecast_next_6_months.csv\"\n",
        "    forecast_df.to_csv(forecast_path, index=False)\n",
        "\n",
        "print(\"✅ RF past predictions + full 6-month future forecasts saved correctly\")"
      ],
      "metadata": {
        "id": "d6rqXQJp7P95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#arima"
      ],
      "metadata": {
        "id": "ceOBTk3CNzlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "BASE_PATH = \"/content/drive/MyDrive/com724_1/models\"\n",
        "os.makedirs(BASE_PATH, exist_ok=True)\n",
        "\n",
        "MAX_FORECAST_DAYS = 180\n",
        "coins = rep_df['representative_coin'].unique()\n",
        "\n",
        "for coin in coins:\n",
        "    coin_df = df_4coins[df_4coins['Symbol'] == coin].copy()\n",
        "\n",
        "    coin_df['Date'] = pd.to_datetime(coin_df['Date'])\n",
        "    coin_df = coin_df.sort_values('Date')\n",
        "    coin_df = coin_df.set_index('Date')\n",
        "\n",
        "    coin_df = coin_df.asfreq('D')\n",
        "\n",
        "    coin_df['Close'] = coin_df['Close'].interpolate(method='linear')\n",
        "\n",
        "    close_series = coin_df['Close']\n",
        "\n",
        "    p_value = adfuller(close_series.dropna())[1]\n",
        "    d = 0 if p_value < 0.05 else 1\n",
        "\n",
        "    model = ARIMA(close_series, order=(5, d, 0))\n",
        "    fitted = model.fit()\n",
        "\n",
        "    past_df = pd.DataFrame({\n",
        "        'Date': close_series.index,\n",
        "        'Close': close_series.values,\n",
        "        'arima_fitted_close': fitted.fittedvalues\n",
        "    }).dropna()\n",
        "\n",
        "    past_path = f\"{BASE_PATH}/{coin}_arima_past_predictions.csv\"\n",
        "    past_df.to_csv(past_path, index=False)\n",
        "\n",
        "    forecast = fitted.forecast(steps=MAX_FORECAST_DAYS)\n",
        "\n",
        "    future_dates = pd.date_range(\n",
        "        start=close_series.index[-1] + pd.Timedelta(days=1),\n",
        "        periods=MAX_FORECAST_DAYS,\n",
        "        freq='D'\n",
        "    )\n",
        "\n",
        "    forecast_df = pd.DataFrame({\n",
        "        'Date': future_dates,\n",
        "        'Day_Number': np.arange(1, MAX_FORECAST_DAYS + 1),\n",
        "        'Forecast_Close': forecast.values,\n",
        "        'Symbol': coin\n",
        "    })\n",
        "\n",
        "    forecast_path = f\"{BASE_PATH}/{coin}_arima_forecast_next_6_months.csv\"\n",
        "    forecast_df.to_csv(forecast_path, index=False)\n",
        "\n",
        "print(\"✅ ARIMA completed with explicit daily frequency (no warnings)\")\n"
      ],
      "metadata": {
        "id": "OPa0oYGhN2sI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#lstm"
      ],
      "metadata": {
        "id": "XJrCCsFrO_MM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Input\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "BASE_PATH = \"/content/drive/MyDrive/COM724_1/models\"\n",
        "os.makedirs(BASE_PATH, exist_ok=True)\n",
        "\n",
        "LOOKBACK = 30\n",
        "FORECAST_DAYS = 180\n",
        "EPOCHS = 30\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "coins = rep_df['representative_coin'].unique()\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "def create_sequences(data, lookback):\n",
        "    X, y = [], []\n",
        "    for i in range(lookback, len(data)):\n",
        "        X.append(data[i - lookback:i])\n",
        "        y.append(data[i])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "for coin in coins:\n",
        "    coin_df = df_4coins[df_4coins['Symbol'] == coin].copy()\n",
        "\n",
        "    coin_df['Date'] = pd.to_datetime(coin_df['Date'])\n",
        "    coin_df = coin_df.sort_values('Date')\n",
        "    coin_df = coin_df.set_index('Date').asfreq('D')\n",
        "\n",
        "    coin_df['Close'] = coin_df['Close'].interpolate()\n",
        "\n",
        "    close_values = coin_df[['Close']].values\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_close = scaler.fit_transform(close_values)\n",
        "\n",
        "    X, y = create_sequences(scaled_close, LOOKBACK)\n",
        "\n",
        "    split = int(len(X) * 0.8)\n",
        "    X_train, X_val = X[:split], X[split:]\n",
        "    y_train, y_val = y[:split], y[split:]\n",
        "\n",
        "\n",
        "    model = Sequential([\n",
        "        Input(shape=(LOOKBACK, 1)),\n",
        "        LSTM(64),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='mse'\n",
        "    )\n",
        "\n",
        "    early_stop = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=5,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        callbacks=[early_stop],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    past_preds_scaled = model.predict(X, verbose=0)\n",
        "    past_preds = scaler.inverse_transform(past_preds_scaled)\n",
        "\n",
        "    past_df = coin_df.iloc[LOOKBACK:].copy()\n",
        "    past_df['lstm_predicted_close'] = past_preds.flatten()\n",
        "\n",
        "    past_path = f\"{BASE_PATH}/{coin}_lstm_past_predictions.csv\"\n",
        "    past_df.reset_index()[['Date', 'Close', 'lstm_predicted_close']].to_csv(\n",
        "        past_path, index=False\n",
        "    )\n",
        "\n",
        "    last_sequence = scaled_close[-LOOKBACK:].reshape(1, LOOKBACK, 1)\n",
        "    future_preds = []\n",
        "\n",
        "    for step in range(FORECAST_DAYS):\n",
        "        next_pred_scaled = model.predict(last_sequence, verbose=0)\n",
        "        next_pred = scaler.inverse_transform(next_pred_scaled)[0, 0]\n",
        "        future_preds.append(next_pred)\n",
        "\n",
        "        last_sequence = np.concatenate(\n",
        "            [last_sequence[:, 1:, :], next_pred_scaled.reshape(1, 1, 1)],\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "    future_dates = pd.date_range(\n",
        "        start=coin_df.index[-1] + pd.Timedelta(days=1),\n",
        "        periods=FORECAST_DAYS,\n",
        "        freq='D'\n",
        "    )\n",
        "\n",
        "    forecast_df = pd.DataFrame({\n",
        "        'Date': future_dates,\n",
        "        'Day_Number': np.arange(1, FORECAST_DAYS + 1),\n",
        "        'Forecast_Close': future_preds,\n",
        "        'Symbol': coin\n",
        "    })\n",
        "\n",
        "    forecast_path = f\"{BASE_PATH}/{coin}_lstm_forecast_next_6_months.csv\"\n",
        "    forecast_df.to_csv(forecast_path, index=False)\n",
        "\n",
        "print(\"✅ LSTM completed — ZERO warnings, clean 6-month forecasts saved\")\n"
      ],
      "metadata": {
        "id": "umT-OemUPA_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#prophet"
      ],
      "metadata": {
        "id": "m0oW80wVPUrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from prophet import Prophet\n",
        "\n",
        "BASE_PATH = \"/content/drive/MyDrive/COM724_1/models\"\n",
        "os.makedirs(BASE_PATH, exist_ok=True)\n",
        "\n",
        "FORECAST_DAYS = 180\n",
        "coins = rep_df['representative_coin'].unique()\n",
        "\n",
        "for coin in coins:\n",
        "    coin_df = df_4coins[df_4coins['Symbol'] == coin].copy()\n",
        "\n",
        "    coin_df['Date'] = pd.to_datetime(coin_df['Date'])\n",
        "    coin_df = coin_df.sort_values('Date')\n",
        "\n",
        "    coin_df = (\n",
        "        coin_df\n",
        "        .set_index('Date')\n",
        "        .asfreq('D')\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    coin_df['Close'] = coin_df['Close'].interpolate(method='linear')\n",
        "\n",
        "    prophet_df = coin_df[['Date', 'Close']].rename(\n",
        "        columns={'Date': 'ds', 'Close': 'y'}\n",
        "    )\n",
        "\n",
        "    model = Prophet(\n",
        "        daily_seasonality=True,\n",
        "        weekly_seasonality=True,\n",
        "        yearly_seasonality=False\n",
        "    )\n",
        "\n",
        "    model.fit(prophet_df)\n",
        "\n",
        "    past_forecast = model.predict(prophet_df)\n",
        "\n",
        "    past_df = pd.DataFrame({\n",
        "        'Date': prophet_df['ds'],\n",
        "        'Close': prophet_df['y'],\n",
        "        'prophet_predicted_close': past_forecast['yhat']\n",
        "    })\n",
        "\n",
        "    past_path = f\"{BASE_PATH}/{coin}_prophet_past_predictions.csv\"\n",
        "    past_df.to_csv(past_path, index=False)\n",
        "\n",
        "    future_df = model.make_future_dataframe(\n",
        "        periods=FORECAST_DAYS,\n",
        "        freq='D',\n",
        "        include_history=False\n",
        "    )\n",
        "\n",
        "    future_forecast = model.predict(future_df)\n",
        "\n",
        "    forecast_df = pd.DataFrame({\n",
        "        'Date': future_forecast['ds'],\n",
        "        'Day_Number': np.arange(1, FORECAST_DAYS + 1),\n",
        "        'Forecast_Close': future_forecast['yhat'],\n",
        "        'Symbol': coin\n",
        "    })\n",
        "\n",
        "    forecast_path = f\"{BASE_PATH}/{coin}_prophet_forecast_next_6_months.csv\"\n",
        "    forecast_df.to_csv(forecast_path, index=False)\n",
        "\n",
        "print(\"✅ Prophet completed — clean past predictions and 6-month forecasts saved\")\n"
      ],
      "metadata": {
        "id": "-RyuUo8_PXA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#visualisation"
      ],
      "metadata": {
        "id": "wm5ttbKnRa2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "BASE_PATH = \"/content/drive/MyDrive/COM724_1/models\"\n",
        "coins = rep_df['representative_coin'].unique()\n",
        "\n",
        "models = {\n",
        "    \"Random Forest\": {\n",
        "        \"past\": \"rf_past_predictions\",\n",
        "        \"forecast\": \"rf_forecast_next_6_months\",\n",
        "        \"pred_col\": \"rf_predicted_close\"\n",
        "    },\n",
        "    \"ARIMA\": {\n",
        "        \"past\": \"arima_past_predictions\",\n",
        "        \"forecast\": \"arima_forecast_next_6_months\",\n",
        "        \"pred_col\": \"arima_fitted_close\"\n",
        "    },\n",
        "    \"LSTM\": {\n",
        "        \"past\": \"lstm_past_predictions\",\n",
        "        \"forecast\": \"lstm_forecast_next_6_months\",\n",
        "        \"pred_col\": \"lstm_predicted_close\"\n",
        "    },\n",
        "    \"Prophet\": {\n",
        "        \"past\": \"prophet_past_predictions\",\n",
        "        \"forecast\": \"prophet_forecast_next_6_months\",\n",
        "        \"pred_col\": \"prophet_predicted_close\"\n",
        "    }\n",
        "}\n",
        "\n",
        "for model_name, cfg in models.items():\n",
        "\n",
        "    print(f\"\\n📈 Plotting {model_name} results\")\n",
        "\n",
        "    for coin in coins:\n",
        "\n",
        "        actual_df = pd.read_csv(\n",
        "            f\"{BASE_PATH}/{coin}_rf_past_predictions.csv\",\n",
        "            parse_dates=['Date']\n",
        "        )\n",
        "\n",
        "        past_df = pd.read_csv(\n",
        "            f\"{BASE_PATH}/{coin}_{cfg['past']}.csv\",\n",
        "            parse_dates=['Date']\n",
        "        )\n",
        "\n",
        "        forecast_df = pd.read_csv(\n",
        "            f\"{BASE_PATH}/{coin}_{cfg['forecast']}.csv\",\n",
        "            parse_dates=['Date']\n",
        "        )\n",
        "\n",
        "        plt.figure(figsize=(14, 6))\n",
        "\n",
        "        plt.plot(\n",
        "            actual_df['Date'],\n",
        "            actual_df['Close'],\n",
        "            label=\"Actual\",\n",
        "            color=\"black\",\n",
        "            linewidth=2\n",
        "        )\n",
        "\n",
        "        plt.plot(\n",
        "            past_df['Date'],\n",
        "            past_df[cfg['pred_col']],\n",
        "            label=\"Past Prediction\",\n",
        "            linestyle=\"--\"\n",
        "        )\n",
        "\n",
        "        plt.plot(\n",
        "            forecast_df['Date'],\n",
        "            forecast_df['Forecast_Close'],\n",
        "            label=\"6-Month Forecast\",\n",
        "            linestyle=\":\"\n",
        "        )\n",
        "\n",
        "        plt.title(f\"{coin} — {model_name}\")\n",
        "        plt.xlabel(\"Date\")\n",
        "        plt.ylabel(\"Price\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "EavFmV2RRdG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#evaluation"
      ],
      "metadata": {
        "id": "1SOJaDlinyIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    mean_absolute_error,\n",
        "    mean_squared_error,\n",
        "    r2_score\n",
        ")\n",
        "\n",
        "\n",
        "MODELS_PATH = \"/content/drive/MyDrive/COM724_1/models\"\n",
        "OUTPUT_PATH = os.path.join(MODELS_PATH, \"evaluation_results.csv\")\n",
        "\n",
        "coins = [\"BTC-USD\", \"ETH-USD\", \"XRP-USD\", \"AVAX-USD\"]\n",
        "\n",
        "models = {\n",
        "    \"Random Forest\": {\n",
        "        \"file\": \"rf_past_predictions\",\n",
        "        \"pred_col\": \"rf_predicted_close\"\n",
        "    },\n",
        "    \"ARIMA\": {\n",
        "        \"file\": \"arima_past_predictions\",\n",
        "        \"pred_col\": \"arima_fitted_close\"\n",
        "    },\n",
        "    \"LSTM\": {\n",
        "        \"file\": \"lstm_past_predictions\",\n",
        "        \"pred_col\": \"lstm_predicted_close\"\n",
        "    },\n",
        "    \"Prophet\": {\n",
        "        \"file\": \"prophet_past_predictions\",\n",
        "        \"pred_col\": \"prophet_predicted_close\"\n",
        "    }\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for coin in coins:\n",
        "    for model_name, cfg in models.items():\n",
        "\n",
        "        file_path = os.path.join(\n",
        "            MODELS_PATH,\n",
        "            f\"{coin}_{cfg['file']}.csv\"\n",
        "        )\n",
        "\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        df = df.dropna(subset=[\"Close\", cfg[\"pred_col\"]])\n",
        "\n",
        "        y_true = df[\"Close\"].values\n",
        "        y_pred = df[cfg[\"pred_col\"]].values\n",
        "\n",
        "        mae = mean_absolute_error(y_true, y_pred)\n",
        "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "        mape = np.mean(\n",
        "            np.abs((y_true - y_pred) / np.where(y_true == 0, 1, y_true))\n",
        "        ) * 100\n",
        "\n",
        "        r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "        results.append({\n",
        "            \"Symbol\": coin,\n",
        "            \"Model\": model_name,\n",
        "            \"MAE\": mae,\n",
        "            \"RMSE\": rmse,\n",
        "            \"MAPE (%)\": mape,\n",
        "            \"R2\": r2\n",
        "        })\n",
        "\n",
        "evaluation_df = pd.DataFrame(results)\n",
        "evaluation_df.to_csv(OUTPUT_PATH, index=False)\n",
        "\n",
        "print(\"✅ FULL evaluation completed (MAE, RMSE, MAPE, R²)\")\n",
        "print(\"📄 Saved to models/evaluation_results.csv\")\n"
      ],
      "metadata": {
        "id": "u4jRdDt_nzoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#model outputs"
      ],
      "metadata": {
        "id": "xnuovcybpesO"
      }
    }
  ]
}